# 在了解XGBoost之前

- 了解监督性学习

  监督学习，就是两步，一是定出模型确定参数，二是根据训练数据找出最佳的参数值

- 了解classification and regression trees (CART)决策树
- 如何找出最佳参数？

  需要目标函数来帮助我们来确定参数是否是最佳的
  目标函数通常由损失项+正则项组成：obj(θ)=L(θ)+Ω(θ)
    通常是：obj = ∑i(sigmoid(∑jθj*xij) - yi)^2 + ∑j(θj^2)
  目标函数可以设为MSE（mean squared error）函数：L(θ)=∑i(yi−y^i)2，这个函数通常被用作回归问题（最小二乘法）
  而对数损失函数通常被用做分类问题：L(θ)=∑i[yiln(1+e−y^i)+(1−yi)ln(1+ey^i)]

# 什么是XGBoost

- XGBoost对应的就是一堆CART树
- 这堆树如何做预测？就是将每棵树的预测值加到一起作为最终的预测值
- 为什么选用CART树？
  - 对于分类问题，由于CART树的叶子节点对应的值是一个实际的分数，而非一个确定的类别，这将有利于实现高效的优化算法。
- XGBoost的数学表达
  - K 表示树的棵数
  - F 表示所有可能的CART树
  - f 表示一棵具体的CART树
- XGBoost的目标函数
  - 包含两部分，第一部分就是损失函数，第二部分就是正则项
  - 这里的正则化项由K棵树的正则化项相加而来，一棵树的正则化项是什么？

# XGBoost的训练
  
  - 训练的任务就是通过最小化目标函数来找到最佳的参数组

# 加法训练

  - 所谓加法训练，本质上是一个元算法，适用于所有的加法模型，它是一种启发式算法。
  - 运用加法训练，我们的目标不再是直接优化整个目标函数，这已经被我们证明是行不通的。而是分步骤优化目标函数，首先优化第一棵树，完了之后再优化第二棵树，直至优化完K棵树。
